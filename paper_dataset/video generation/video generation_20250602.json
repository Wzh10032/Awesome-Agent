[
    {
        "title": "AdaHuman: Animatable Detailed 3D Human Generation with Compositional Multiview Diffusion",
        "authors": [
            "Yangyi Huang",
            "Ye Yuan",
            "Xueting Li",
            "Jan Kautz",
            "Umar Iqbal"
        ],
        "abstract": "Existing methods for image-to-3D avatar generation struggle to produce highly\ndetailed, animation-ready avatars suitable for real-world applications. We\nintroduce AdaHuman, a novel framework that generates high-fidelity animatable\n3D avatars from a single in-the-wild image. AdaHuman incorporates two key\ninnovations: (1) A pose-conditioned 3D joint diffusion model that synthesizes\nconsistent multi-view images in arbitrary poses alongside corresponding 3D\nGaussian Splats (3DGS) reconstruction at each diffusion step; (2) A\ncompositional 3DGS refinement module that enhances the details of local body\nparts through image-to-image refinement and seamlessly integrates them using a\nnovel crop-aware camera ray map, producing a cohesive detailed 3D avatar. These\ncomponents allow AdaHuman to generate highly realistic standardized A-pose\navatars with minimal self-occlusion, enabling rigging and animation with any\ninput motion. Extensive evaluation on public benchmarks and in-the-wild images\ndemonstrates that AdaHuman significantly outperforms state-of-the-art methods\nin both avatar reconstruction and reposing. Code and models will be publicly\navailable for research purposes.",
        "published": "2025-05-30 17:59:54",
        "updated": "2025-05-30 17:59:54",
        "arxiv_id": "2505.24877v1",
        "pdf_url": "http://arxiv.org/pdf/2505.24877v1",
        "filtered": false,
        "classification": ""
    },
    {
        "title": "Agent-X: Evaluating Deep Multimodal Reasoning in Vision-Centric Agentic Tasks",
        "authors": [
            "Tajamul Ashraf",
            "Amal Saqib",
            "Hanan Ghani",
            "Muhra AlMahri",
            "Yuhao Li",
            "Noor Ahsan",
            "Umair Nawaz",
            "Jean Lahoud",
            "Hisham Cholakkal",
            "Mubarak Shah",
            "Philip Torr",
            "Fahad Shahbaz Khan",
            "Rao Muhammad Anwer",
            "Salman Khan"
        ],
        "abstract": "Deep reasoning is fundamental for solving complex tasks, especially in\nvision-centric scenarios that demand sequential, multimodal understanding.\nHowever, existing benchmarks typically evaluate agents with fully synthetic,\nsingle-turn queries, limited visual modalities, and lack a framework to assess\nreasoning quality over multiple steps as required in real-world settings. To\naddress this, we introduce Agent-X, a large-scale benchmark for evaluating\nvision-centric agents multi-step and deep reasoning capabilities in real-world,\nmultimodal settings. Agent- X features 828 agentic tasks with authentic visual\ncontexts, including images, multi-image comparisons, videos, and instructional\ntext. These tasks span six major agentic environments: general visual\nreasoning, web browsing, security and surveillance, autonomous driving, sports,\nand math reasoning. Our benchmark requires agents to integrate tool use with\nexplicit, stepwise decision-making in these diverse settings. In addition, we\npropose a fine-grained, step-level evaluation framework that assesses the\ncorrectness and logical coherence of each reasoning step and the effectiveness\nof tool usage throughout the task. Our results reveal that even the\nbest-performing models, including GPT, Gemini, and Qwen families, struggle to\nsolve multi-step vision tasks, achieving less than 50% full-chain success.\nThese findings highlight key bottlenecks in current LMM reasoning and tool-use\ncapabilities and identify future research directions in vision-centric agentic\nreasoning models. Our data and code are publicly available at\nhttps://github.com/mbzuai-oryx/Agent-X",
        "published": "2025-05-30 17:59:53",
        "updated": "2025-05-30 17:59:53",
        "arxiv_id": "2505.24876v1",
        "pdf_url": "http://arxiv.org/pdf/2505.24876v1",
        "filtered": false,
        "classification": ""
    },
    {
        "title": "ReasonGen-R1: CoT for Autoregressive Image generation models through SFT and RL",
        "authors": [
            "Yu Zhang",
            "Yunqi Li",
            "Yifan Yang",
            "Rui Wang",
            "Yuqing Yang",
            "Dai Qi",
            "Jianmin Bao",
            "Dongdong Chen",
            "Chong Luo",
            "Lili Qiu"
        ],
        "abstract": "Although chain-of-thought reasoning and reinforcement learning (RL) have\ndriven breakthroughs in NLP, their integration into generative vision models\nremains underexplored. We introduce ReasonGen-R1, a two-stage framework that\nfirst imbues an autoregressive image generator with explicit text-based\n\"thinking\" skills via supervised fine-tuning on a newly generated reasoning\ndataset of written rationales, and then refines its outputs using Group\nRelative Policy Optimization. To enable the model to reason through text before\ngenerating images, We automatically generate and release a corpus of model\ncrafted rationales paired with visual prompts, enabling controlled planning of\nobject layouts, styles, and scene compositions. Our GRPO algorithm uses reward\nsignals from a pretrained vision language model to assess overall visual\nquality, optimizing the policy in each update. Evaluations on GenEval, DPG, and\nthe T2I benchmark demonstrate that ReasonGen-R1 consistently outperforms strong\nbaselines and prior state-of-the-art models. More: aka.ms/reasongen.",
        "published": "2025-05-30 17:59:48",
        "updated": "2025-05-30 17:59:48",
        "arxiv_id": "2505.24875v1",
        "pdf_url": "http://arxiv.org/pdf/2505.24875v1",
        "filtered": false,
        "classification": ""
    },
    {
        "title": "The Road to Generalizable Neuro-Symbolic Learning Should be Paved with Foundation Models",
        "authors": [
            "Adam Stein",
            "Aaditya Naik",
            "Neelay Velingker",
            "Mayur Naik",
            "Eric Wong"
        ],
        "abstract": "Neuro-symbolic learning was proposed to address challenges with training\nneural networks for complex reasoning tasks with the added benefits of\ninterpretability, reliability, and efficiency. Neuro-symbolic learning methods\ntraditionally train neural models in conjunction with symbolic programs, but\nthey face significant challenges that limit them to simplistic problems. On the\nother hand, purely-neural foundation models now reach state-of-the-art\nperformance through prompting rather than training, but they are often\nunreliable and lack interpretability. Supplementing foundation models with\nsymbolic programs, which we call neuro-symbolic prompting, provides a way to\nuse these models for complex reasoning tasks. Doing so raises the question:\nWhat role does specialized model training as part of neuro-symbolic learning\nhave in the age of foundation models? To explore this question, we highlight\nthree pitfalls of traditional neuro-symbolic learning with respect to the\ncompute, data, and programs leading to generalization problems. This position\npaper argues that foundation models enable generalizable neuro-symbolic\nsolutions, offering a path towards achieving the original goals of\nneuro-symbolic learning without the downsides of training from scratch.",
        "published": "2025-05-30 17:59:46",
        "updated": "2025-05-30 17:59:46",
        "arxiv_id": "2505.24874v1",
        "pdf_url": "http://arxiv.org/pdf/2505.24874v1",
        "filtered": false,
        "classification": ""
    },
    {
        "title": "MiniMax-Remover: Taming Bad Noise Helps Video Object Removal",
        "authors": [
            "Bojia Zi",
            "Weixuan Peng",
            "Xianbiao Qi",
            "Jianan Wang",
            "Shihao Zhao",
            "Rong Xiao",
            "Kam-Fai Wong"
        ],
        "abstract": "Recent advances in video diffusion models have driven rapid progress in video\nediting techniques. However, video object removal, a critical subtask of video\nediting, remains challenging due to issues such as hallucinated objects and\nvisual artifacts. Furthermore, existing methods often rely on computationally\nexpensive sampling procedures and classifier-free guidance (CFG), resulting in\nslow inference. To address these limitations, we propose MiniMax-Remover, a\nnovel two-stage video object removal approach. Motivated by the observation\nthat text condition is not best suited for this task, we simplify the\npretrained video generation model by removing textual input and cross-attention\nlayers, resulting in a more lightweight and efficient model architecture in the\nfirst stage. In the second stage, we distilled our remover on successful videos\nproduced by the stage-1 model and curated by human annotators, using a minimax\noptimization strategy to further improve editing quality and inference speed.\nSpecifically, the inner maximization identifies adversarial input noise (\"bad\nnoise\") that makes failure removals, while the outer minimization step trains\nthe model to generate high-quality removal results even under such challenging\nconditions. As a result, our method achieves a state-of-the-art video object\nremoval results with as few as 6 sampling steps and doesn't rely on CFG,\nsignificantly improving inference efficiency. Extensive experiments demonstrate\nthe effectiveness and superiority of MiniMax-Remover compared to existing\nmethods. Codes and Videos are available at: https://minimax-remover.github.io.",
        "published": "2025-05-30 17:59:45",
        "updated": "2025-05-30 17:59:45",
        "arxiv_id": "2505.24873v1",
        "pdf_url": "http://arxiv.org/pdf/2505.24873v1",
        "filtered": false,
        "classification": ""
    },
    {
        "title": "MoDoMoDo: Multi-Domain Data Mixtures for Multimodal LLM Reinforcement Learning",
        "authors": [
            "Yiqing Liang",
            "Jielin Qiu",
            "Wenhao Ding",
            "Zuxin Liu",
            "James Tompkin",
            "Mengdi Xu",
            "Mengzhou Xia",
            "Zhengzhong Tu",
            "Laixi Shi",
            "Jiacheng Zhu"
        ],
        "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as\na powerful paradigm for post-training large language models (LLMs), achieving\nstate-of-the-art performance on tasks with structured, verifiable answers.\nApplying RLVR to Multimodal LLMs (MLLMs) presents significant opportunities but\nis complicated by the broader, heterogeneous nature of vision-language tasks\nthat demand nuanced visual, logical, and spatial capabilities. As such,\ntraining MLLMs using RLVR on multiple datasets could be beneficial but creates\nchallenges with conflicting objectives from interaction among diverse datasets,\nhighlighting the need for optimal dataset mixture strategies to improve\ngeneralization and reasoning. We introduce a systematic post-training framework\nfor Multimodal LLM RLVR, featuring a rigorous data mixture problem formulation\nand benchmark implementation. Specifically, (1) We developed a multimodal RLVR\nframework for multi-dataset post-training by curating a dataset that contains\ndifferent verifiable vision-language problems and enabling multi-domain online\nRL learning with different verifiable rewards; (2) We proposed a data mixture\nstrategy that learns to predict the RL fine-tuning outcome from the data\nmixture distribution, and consequently optimizes the best mixture.\nComprehensive experiments showcase that multi-domain RLVR training, when\ncombined with mixture prediction strategies, can significantly boost MLLM\ngeneral reasoning capacities. Our best mixture improves the post-trained\nmodel's accuracy on out-of-distribution benchmarks by an average of 5.24%\ncompared to the same model post-trained with uniform data mixture, and by a\ntotal of 20.74% compared to the pre-finetuning baseline.",
        "published": "2025-05-30 17:59:38",
        "updated": "2025-05-30 17:59:38",
        "arxiv_id": "2505.24871v1",
        "pdf_url": "http://arxiv.org/pdf/2505.24871v1",
        "filtered": false,
        "classification": ""
    },
    {
        "title": "GenSpace: Benchmarking Spatially-Aware Image Generation",
        "authors": [
            "Zehan Wang",
            "Jiayang Xu",
            "Ziang Zhang",
            "Tianyu Pan",
            "Chao Du",
            "Hengshuang Zhao",
            "Zhou Zhao"
        ],
        "abstract": "Humans can intuitively compose and arrange scenes in the 3D space for\nphotography. However, can advanced AI image generators plan scenes with similar\n3D spatial awareness when creating images from text or image prompts? We\npresent GenSpace, a novel benchmark and evaluation pipeline to comprehensively\nassess the spatial awareness of current image generation models. Furthermore,\nstandard evaluations using general Vision-Language Models (VLMs) frequently\nfail to capture the detailed spatial errors. To handle this challenge, we\npropose a specialized evaluation pipeline and metric, which reconstructs 3D\nscene geometry using multiple visual foundation models and provides a more\naccurate and human-aligned metric of spatial faithfulness. Our findings show\nthat while AI models create visually appealing images and can follow general\ninstructions, they struggle with specific 3D details like object placement,\nrelationships, and measurements. We summarize three core limitations in the\nspatial perception of current state-of-the-art image generation models: 1)\nObject Perspective Understanding, 2) Egocentric-Allocentric Transformation and\n3) Metric Measurement Adherence, highlighting possible directions for improving\nspatial intelligence in image generation.",
        "published": "2025-05-30 17:59:26",
        "updated": "2025-05-30 17:59:26",
        "arxiv_id": "2505.24870v1",
        "pdf_url": "http://arxiv.org/pdf/2505.24870v1",
        "filtered": false,
        "classification": ""
    },
    {
        "title": "SiLVR: A Simple Language-based Video Reasoning Framework",
        "authors": [
            "Ce Zhang",
            "Yan-Bo Lin",
            "Ziyang Wang",
            "Mohit Bansal",
            "Gedas Bertasius"
        ],
        "abstract": "Recent advances in test-time optimization have led to remarkable reasoning\ncapabilities in Large Language Models (LLMs), enabling them to solve highly\ncomplex problems in math and coding. However, the reasoning capabilities of\nmultimodal LLMs (MLLMs) still significantly lag, especially for complex\nvideo-language tasks. To address this issue, we present SiLVR, a Simple\nLanguage-based Video Reasoning framework that decomposes complex video\nunderstanding into two stages. In the first stage, SiLVR transforms raw video\ninto language-based representations using multisensory inputs, such as short\nclip captions and audio/speech subtitles. In the second stage, language\ndescriptions are fed into a powerful reasoning LLM to solve complex\nvideo-language understanding tasks. To handle long-context multisensory inputs,\nwe use an adaptive token reduction scheme, which dynamically determines the\ntemporal granularity with which to sample the tokens. Our simple, modular, and\ntraining-free video reasoning framework achieves the best-reported results on\nVideo-MME (long), Video-MMMU (comprehension), Video-MMLU, CGBench, and EgoLife.\nFurthermore, our empirical study focused on video reasoning capabilities shows\nthat, despite not being explicitly trained on video, strong reasoning LLMs can\neffectively aggregate multisensory input information from video, speech, and\naudio for complex temporal, causal, long-context, and knowledge acquisition\nreasoning tasks in video. Code is available at https://github.com/CeeZh/SILVR.",
        "published": "2025-05-30 17:59:19",
        "updated": "2025-05-30 17:59:19",
        "arxiv_id": "2505.24869v1",
        "pdf_url": "http://arxiv.org/pdf/2505.24869v1",
        "filtered": false,
        "classification": ""
    },
    {
        "title": "Consistent line clustering using geometric hypergraphs",
        "authors": [
            "Kalle Alaluusua",
            "Konstantin Avrachenkov",
            "B. R. Vinay Kumar",
            "Lasse Leskel√§"
        ],
        "abstract": "Traditional data analysis often represents data as a weighted graph with\npairwise similarities, but many problems do not naturally fit this framework.\nIn line clustering, points in a Euclidean space must be grouped so that each\ncluster is well approximated by a line segment. Since any two points define a\nline, pairwise similarities fail to capture the structure of the problem,\nnecessitating the use of higher-order interactions modeled by geometric\nhypergraphs. We encode geometry into a 3-uniform hypergraph by treating sets of\nthree points as hyperedges whenever they are approximately collinear. The\nresulting hypergraph contains information about the underlying line segments,\nwhich can then be extracted using community recovery algorithms. In contrast to\nclassical hypergraph block models, latent geometric constraints in this\nconstruction introduce significant dependencies between hyperedges, which\nrestricts the applicability of many standard theoretical tools. We aim to\ndetermine the fundamental limits of line clustering and evaluate\nhypergraph-based line clustering methods. To this end, we derive\ninformation-theoretic thresholds for exact and almost exact recovery for data\ngenerated from intersecting lines on a plane with additive Gaussian noise. We\ndevelop a polynomial-time spectral algorithm and show that it succeeds under\nnoise conditions that match the information-theoretic bounds up to a\npolylogarithmic factor.",
        "published": "2025-05-30 17:59:17",
        "updated": "2025-05-30 17:59:17",
        "arxiv_id": "2505.24868v1",
        "pdf_url": "http://arxiv.org/pdf/2505.24868v1",
        "filtered": false,
        "classification": ""
    },
    {
        "title": "Time Blindness: Why Video-Language Models Can't See What Humans Can?",
        "authors": [
            "Ujjwal Upadhyay",
            "Mukul Ranjan",
            "Zhiqiang Shen",
            "Mohamed Elhoseiny"
        ],
        "abstract": "Recent advances in vision-language models (VLMs) have made impressive strides\nin understanding spatio-temporal relationships in videos. However, when spatial\ninformation is obscured, these models struggle to capture purely temporal\npatterns. We introduce $\\textbf{SpookyBench}$, a benchmark where information is\nencoded solely in temporal sequences of noise-like frames, mirroring natural\nphenomena from biological signaling to covert communication. Interestingly,\nwhile humans can recognize shapes, text, and patterns in these sequences with\nover 98% accuracy, state-of-the-art VLMs achieve 0% accuracy. This performance\ngap highlights a critical limitation: an over-reliance on frame-level spatial\nfeatures and an inability to extract meaning from temporal cues. Furthermore,\nwhen trained in data sets with low spatial signal-to-noise ratios (SNR),\ntemporal understanding of models degrades more rapidly than human perception,\nespecially in tasks requiring fine-grained temporal reasoning. Overcoming this\nlimitation will require novel architectures or training paradigms that decouple\nspatial dependencies from temporal processing. Our systematic analysis shows\nthat this issue persists across model scales and architectures. We release\nSpookyBench to catalyze research in temporal pattern recognition and bridge the\ngap between human and machine video understanding. Dataset and code has been\nmade available on our project website: https://timeblindness.github.io/.",
        "published": "2025-05-30 17:59:12",
        "updated": "2025-05-30 17:59:12",
        "arxiv_id": "2505.24867v1",
        "pdf_url": "http://arxiv.org/pdf/2505.24867v1",
        "filtered": false,
        "classification": ""
    }
]