import os
import json
import re
from datetime import datetime
from urllib.parse import urlparse

def extract_code_link(abstract):
    """ä»abstractä¸­æå–å¹¶æ¸…ç†å¼€æºä»£ç é“¾æ¥"""
    # åŒ¹é…æ‰€æœ‰å¯èƒ½çš„URLæ¨¡å¼ï¼ˆåŒ…æ‹¬ä¸­é—´æœ‰ç©ºæ ¼çš„ï¼‰
    url_patterns = [
        r'https?:\s*//\S+',  # å¤„ç†åè®®éƒ¨åˆ†æœ‰ç©ºæ ¼çš„æƒ…å†µ
        r'http\s*://\S+',     # å¤„ç†httpåè·Ÿç©ºæ ¼çš„æƒ…å†µ
        r'www\.\S+\.\S+',     # åŒ¹é…wwwå¼€å¤´çš„ç½‘å€
        r'\S+\.(com|org|net|io|ai|github|gitlab)\S*'  # åŒ¹é…å¸¸è§åŸŸå
    ]
    
    # å°è¯•æ‰€æœ‰æ¨¡å¼ï¼Œç›´åˆ°æ‰¾åˆ°åŒ¹é…é¡¹
    for pattern in url_patterns:
        matches = re.findall(pattern, abstract)
        if matches:
            # é€‰æ‹©æœ€é•¿çš„åŒ¹é…é¡¹ï¼ˆé€šå¸¸æ˜¯æœ€å®Œæ•´çš„é“¾æ¥ï¼‰
            raw_url = max(matches, key=len)
            
            # åŸºæœ¬æ¸…ç†ï¼šç§»é™¤æ‰€æœ‰ç©ºæ ¼å’Œå¸¸è§ç»“å°¾æ ‡ç‚¹
            clean_url = raw_url.replace(' ', '')
            clean_url = re.sub(r'[.,;:!?)\'"]+$', '', clean_url)
            
            # éªŒè¯æ˜¯å¦ä¸ºæœ‰æ•ˆURL
            if is_valid_url(clean_url):
                return clean_url
    
    return None

def is_valid_url(url):
    """æ£€æŸ¥æ˜¯å¦ä¸ºæœ‰æ•ˆURL"""
    try:
        result = urlparse(url)
        # åŸºæœ¬éªŒè¯ï¼šéœ€è¦åè®®å’Œç½‘ç»œä½ç½®
        return all([result.scheme, result.netloc])
    except:
        return False

def generate_readme(json_path, output_path="README.md"):
    # è¯»å–JSONæ–‡ä»¶
    print(f"JSONè·¯å¾„: {json_path}")
    with open(json_path, 'r', encoding='utf-8') as f:
        papers = json.load(f)
    
    # è·å–åˆ†ç±»åç§°
    category_name = json_path.split(os.sep)[-2] if os.sep in json_path else "Research Papers"
    
    # æŒ‰åˆ†ç±»åˆ†ç»„
    categories = {}
    for paper in papers:
        category = paper.get("classification", "Uncategorized")
        categories.setdefault(category, []).append(paper)
    
    # ç”ŸæˆMarkdownå†…å®¹
    md_content = f"# ğŸ“š Awesome {category_name} Research\n\n"
    md_content += f"> ğŸ—“ï¸ è‡ªåŠ¨ç”Ÿæˆçš„è®ºæ–‡åˆ—è¡¨ | æœ€åæ›´æ–°: {datetime.now().strftime('%Y-%m-%d')}\n\n"
    
    # ç›®å½•ç”Ÿæˆ
    md_content += "## ğŸ” ç›®å½•\n"
    for category in sorted(categories.keys()):
        md_content += f"- [{category}](#{category.lower().replace(' ', '-')})\n"
    md_content += "\n---\n\n"
    
    # æŒ‰åˆ†ç±»æ·»åŠ è®ºæ–‡
    for category, papers_in_category in sorted(categories.items()):
        md_content += f"## ğŸ“ {category}\n\n"
        
        # æŒ‰å‘å¸ƒæ—¥æœŸæ’åºï¼ˆæœ€æ–°ä¼˜å…ˆï¼‰
        papers_in_category.sort(key=lambda x: x["published"], reverse=True)
        
        for paper in papers_in_category:
            # æ ¼å¼åŒ–æ—¥æœŸ
            pub_date = datetime.strptime(paper["published"], "%Y-%m-%d %H:%M:%S").strftime("%b %d, %Y")
            update_date = datetime.strptime(paper["updated"], "%Y-%m-%d %H:%M:%S").strftime("%b %d, %Y")
            
            md_content += f"### ğŸ“„ [{paper['title']}]({paper['pdf_url']})\n\n"
            md_content += f"ğŸ‘¤ **Authors**: {', '.join(paper['authors'])}\n\n"
            md_content += f"ğŸ“… **Published**: `{pub_date}` | ğŸ”„ **Updated**: `{update_date}` | "
            md_content += f"ğŸ†” **Arxiv ID**: [{paper['arxiv_id']}]({paper['pdf_url']})\n\n"
            
            # æå–å¼€æºä»£ç é“¾æ¥
            code_link = extract_code_link(paper['abstract'])
            if code_link:
                # ç¡®ä¿URLæœ‰åè®®å‰ç¼€
                if not code_link.startswith('http'):
                    code_link = 'https://' + code_link.lstrip('/')
                md_content += f"ğŸ’» **Code**: [Link]({code_link})\n\n"
            
            md_content += "ğŸ¤– **AI Summary**:\n> " + paper['summary'].replace('\n', '\n> ') + "\n\n"
            md_content += "---\n\n"
    
    # å†™å…¥æ–‡ä»¶
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(md_content)
    
    print(f"README ç”ŸæˆæˆåŠŸ: {output_path}")
# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    generate_readme("D:\code\Awesome-agent\paper_dataset\Object detection\Object detection_20250614.json", output_path="Object detection.md")